# -*- coding: utf-8 -*-
"""admissions

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BT-uJWMGwkLVtaxT4JoVBW-ZnbeR9yC4
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df=pd.read_csv("/content/Admission_Predict_Ver1.1.csv")
df.shape
df.isnull().sum()
df.columns
df=df.drop('SerialNo', axis = 1)
y = (df['Admit'] >= 0.75).astype(int)
df_without_outcome = df.drop("Admit", axis = 1)
from sklearn.decomposition import PCA
pca = PCA()
pca.fit_transform(df_without_outcome)
pca.get_covariance()
explained_variance = pca.explained_variance_ratio_
explained_variance

loadings = pd.DataFrame(
    pca.components_,  # each row = PC, each column = original feature weights
    columns=df_without_outcome.columns,
    index=[f'PC{i+1}' for i in range(len(df_without_outcome.columns))]
)

# Show contributions for all PCs
print(loadings)

# Optional: just see PC1 contributions
print("\nFeature contributions to PC1:")
print(loadings.loc['PC1'].sort_values(ascending=False))

df=df[['GRE', 'TOEFL', 'CGPA', 'SOP']]
#preprocessing/normalizing
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 10))
df[['GRE', 'TOEFL', 'CGPA', 'SOP']] = scaler.fit_transform(df[['GRE', 'TOEFL', 'CGPA', 'SOP']])
df

#bins
#sop
Newsop = pd.Series(["verypoor", "poor", "Mid","good","impressive"], dtype = "category")
df['Newsop'] = Newsop
df.loc[df["SOP"] <= 2, "Newsop"] = Newsop[0]
df.loc[(df["SOP"] > 2) & (df["SOP"] <= 4), "Newsop"]=Newsop[1]
df.loc[(df["SOP"] > 4) & (df["SOP"] <= 6), "Newsop"]=Newsop[2]
df.loc[(df["SOP"] > 6) & (df["SOP"] <= 8), "Newsop"]=Newsop[3]
df.loc[df["SOP"] > 8 ,"Newsop"] = Newsop[4]
df.head()

fig, ax = plt.subplots(figsize=(8, 6))
df['Newsop'].hist(ax=ax, edgecolor="black")
ax.set_title("SOP Categories")
plt.show()

df = df[['GRE', 'TOEFL', 'CGPA', 'SOP']]
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
xtrain,xtest,ytrain,ytest = train_test_split(df,y,test_size=0.4, random_state=0)
model = LogisticRegression()
model.fit(xtrain,ytrain)
ypred = model.predict(xtest)
ypred

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
print('Model accuracy score: {0:0.4f}'.
format(accuracy_score(ytest, ypred)))
print("Confusion Matrix:\n", confusion_matrix(ytest, ypred))
print("Classification Report:\n", classification_report(ytest, ypred))

import pandas as pd

# Sample new student data â€” same feature order as your training data
sample_data = pd.DataFrame({
    'GRE': [330, 310, 290],
    'TOEFL': [115, 100, 90],
    'CGPA': [9.2, 8.0, 7.0],
    'SOP': [4.5, 3.0, 2.0]
})

# Make sure columns are in same order as model training
sample_data = sample_data[['GRE', 'TOEFL', 'CGPA', 'SOP']]

# Predict admission outcome
sample_predictions = model.predict(sample_data)

# If you used logistic regression (binary admit / not admit):
print("Predicted Admission (1 = Admit, 0 = Not Admit):")
print(sample_predictions)

# If you want probabilities:
if hasattr(model, "predict_proba"):
    sample_probabilities = model.predict_proba(sample_data)
    print("\nAdmission Probability:")
    print(sample_probabilities)